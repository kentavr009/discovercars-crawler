# discovercars-crawler
# DiscoverCars Crawler

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Многопоточный BFS-краулер для сбора данных о точках аренды автомобилей с сайта `discovercars.com`. Скрипт использует метод обхода в ширину (BFS), итеративно углубляя префиксы поиска (`a`, `aa`, `ab`...) для получения полного списка локаций через внутренний API автодополнения сайта.

## Ключевые особенности

-   **Многопоточность**: Значительное ускорение сбора данных за счет одновременной работы нескольких потоков.
-   **Поддержка прокси**: Каждому потоку назначается свой прокси-сервер из списка `proxies.txt` для обхода ограничений и банов по IP.
-   **Возобновление работы**: Скрипт автоматически сохраняет свое состояние (очередь префиксов и найденные ID) и может быть перезапущен после остановки, продолжая работу с того же места.
-   **Надежность**: Встроенная логика повторных запросов (`Retry`) при сетевых ошибках или временной недоступности сервера.
-   **Двойной формат вывода**: Результаты сохраняются одновременно в `CSV` для табличного анализа и в `JSONL` для гибкой обработки данных.
-   **Интерактивный интерфейс**: Наглядное отображение прогресса с помощью `rich` и `tqdm`.

## Установка

1.  Клонируйте репозиторий:
    ```bash
    git clone https://github.com/ВАШ_НИК/discovercars-crawler.git
    cd discovercars-crawler
    ```

2.  (Рекомендуется) Создайте и активируйте виртуальное окружение:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Для Windows: venv\Scripts\activate
    ```

3.  Установите необходимые зависимости:
    ```bash
    pip install -r requirements.txt
    ```

## Конфигурация

Перед первым запуском необходимо выполнить два шага:

1.  **Создать файл `proxies.txt`** в корневой папке проекта. Добавьте в него ваши прокси-серверы, по одному на строку, в формате `protocol://user:password@ip:port` или `protocol://ip:port`.

    *Пример `proxies.txt`:*
    ```
    http://user1:pass1@192.168.1.1:8000
    http://user2:pass2@192.168.1.2:8000
    https://192.168.1.3:9000
    ```

2.  **Настроить константы в `crawler.py`** (по желанию):
    -   `N_THREADS`: Количество потоков. Рекомендуется ставить равным количеству ваших прокси.
    -   `MAX_DEPTH`: Максимальная глубина префикса (например, `4` означает `abcd`).
    -   `DELAY`: Задержка между запросами в одном потоке для снижения нагрузки.
    -   `BACKUP_EVERY`: Как часто сохранять состояние (по умолчанию раз в 2 минуты).

## Использование

Запустите скрипт из корневой папки проекта:

```bash
python3 crawler.py
```

Вы увидите интерактивную таблицу с текущим состоянием процесса:

```
✓ 1,234   ★ 5,678   ⏳ 9,012
```
*   `✓` — обработано префиксов
*   `★` — найдено уникальных точек
*   `⏳` — префиксов в очереди на обработку

Скрипт можно безопасно остановить в любой момент (`Ctrl+C`). При следующем запуске он автоматически загрузит последнее сохраненное состояние из `cache/queue_state.pkl` и продолжит работу.

### Результаты

-   **`out/discovercars_live.csv`**: Табличные данные, готовые для импорта в Excel/Google Sheets.
-   **`out/discovercars_live.jsonl`**: Данные в формате JSON Lines, удобном для программной обработки.
-   **`cache/queue_state.pkl`**: Файл состояния для возобновления работы.

## Структура проекта

```
.
├── cache/                # Директория для кэша и состояния
│   └── queue_state.pkl
├── out/                  # Директория для результатов
│   ├── discovercars_live.csv
│   └── discovercars_live.jsonl
├── .gitignore
├── crawler.py            # Основной скрипт парсера
├── LICENSE
├── proxies.txt           # Ваш файл с прокси (необходимо создать)
└── README.md
```

## Лицензия

Этот проект распространяется под лицензией MIT. Подробности смотрите в файле [LICENSE](LICENSE).
